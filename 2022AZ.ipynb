{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as Xet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arizona 2022 General Election PBER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load election results\n",
    "\n",
    "Data from openelections, can be downloaded from Github at https://github.com/openelections/openelections-data-az/tree/master/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './20221108__az__general__precinct.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m election_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m election_results\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './20221108__az__general__precinct.csv'"
     ]
    }
   ],
   "source": [
    "csv_path = './20221108__az__general__precinct.csv'\n",
    "pd.set_option('display.max_rows', 0)\n",
    "pd.set_option('display.max_columns', 0)\n",
    "election_results = pd.read_csv(csv_path)\n",
    "election_results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dataset from MEDSL, just for double check, in cases where openelection data does not seem correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_az = '../AZ-cleaned.csv'\n",
    "election_results_MEDSL = pd.read_csv(csv_path_az)\n",
    "election_results_MEDSL.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_before = election_results.shape[0]\n",
    "\n",
    "# Formate votes to numbers\n",
    "election_results['votes'] = pd.to_numeric(election_results['votes'], errors='coerce')\n",
    "\n",
    "# Remove rows without a candidate\n",
    "election_results_cleaned = election_results.dropna(subset=['candidate'])\n",
    "\n",
    "rows_after = election_results_cleaned.shape[0]\n",
    "\n",
    "# Remove rows without a valid vote\n",
    "election_results_cleaned = election_results_cleaned[election_results_cleaned['votes'] > 0]\n",
    "\n",
    "print(f\"Number of rows before cleaning: {rows_before}\")\n",
    "print(f\"Number of rows after cleaning: {rows_after}\")\n",
    "\n",
    "# Convert data types \n",
    "election_results_cleaned['votes'] = election_results_cleaned['votes'].astype(int)\n",
    "\n",
    "# Display the cleaned data\n",
    "election_results_cleaned.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to swap the parts of the name\n",
    "def swap_name(name):\n",
    "    # add a try catach\n",
    "    try:\n",
    "        parts = name.split(',')\n",
    "        if len(parts) == 2:\n",
    "            return parts[1].strip() + ' ' + parts[0].strip()\n",
    "    except:\n",
    "        return name\n",
    "    return name\n",
    "\n",
    "# Convert the 'party' column to strings, handling NaN values by filling them with an empty string or a placeholder\n",
    "election_results_cleaned['party'] = election_results_cleaned['party'].astype(str)\n",
    "\n",
    "# Clean and format\n",
    "election_results_cleaned['party'] = election_results_cleaned['party'].str.strip().str.upper()\n",
    "election_results_cleaned['office'] = election_results_cleaned['office'].str.strip().str.upper()\n",
    "election_results_cleaned['candidate'] = election_results_cleaned['candidate'].str.strip().str.upper()\n",
    "election_results_cleaned['candidate'] = election_results_cleaned['candidate'].apply(swap_name)\n",
    "\n",
    "# Display the unique values for 'office', 'candidate', and 'party'\n",
    "unique_offices = sorted(election_results_cleaned['office'].unique())\n",
    "unique_candidates = sorted(election_results_cleaned['candidate'].unique())\n",
    "unique_parties = sorted(election_results_cleaned['party'].unique())\n",
    "\n",
    "print(f\"Unique values in 'office':\\n{unique_offices}\")\n",
    "print(f\"\\nUnique values in 'candidate':\\n{unique_candidates}\")\n",
    "print(f\"\\nUnique values in 'party':\\n{unique_parties}\")\n",
    "\n",
    "# Count the number of unique candidates for each office\n",
    "unique_candidates_per_office = election_results_cleaned.groupby('office')['candidate'].nunique()\n",
    "\n",
    "print(\"Number of unique candidates for each office:\")\n",
    "print(unique_candidates_per_office)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at the values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For values in 'office', they seem pretty correct  \n",
    "  \n",
    "For candidates, there exist: 'WRITE-IN', 'WRITE-INS', 'UNDER VOTES', 'OVER VOTES', 'NOT QUALIFIED'  \n",
    "And these names look suspicious:  \n",
    "'T., TOM'  \n",
    "'QUEZADA, MARTÍN'  \n",
    "'MERRILL, RAYSHAWN D'ANTHONY \"SHAWN\"'  \n",
    "'MARTÍN QUEZADA' should be 'MARTÍN QUEZADA'  \n",
    "'MARKY KELLY' should be 'MARK KELLY'  \n",
    "'LUTES-BURTON, MIKAELA SHONNIE \"MIKKI\"'  \n",
    "'JACQUELINE PARKER', 'JACUELINE PARKER'  \n",
    "'HODGE, JEVIN D.'  \n",
    "'HANS, CYNTHIA \"CINDY\"'  \n",
    "'HAMADEH, ABRAHAM \"ABE\"'  \n",
    "'GRIJALVA, RAUL', 'GRIJALVA, RAUL 32', 'GRIJALVA, RAÚL', 'GRIJALVA, RAÖL', 'GRIJALVA, RAÖL 160'  \n",
    "'GARCIA SNYDER, GARY', 'GARY GARCIA SNYDER'  \n",
    "'GABALDON, ROSANNA', 'GABALDÓN, ROSANNA'  \n",
    "'EPSTEIN, DENISE \"MITZI\"'  \n",
    "'DUNN, TIMOTHY \"TIM\"'  \n",
    "'DI GENOVA, TRISTA', 'DI GENOVA, TRISTA \"TRISTA\"'  \n",
    "'DENNY, STEPH NOELLE \"STEPH\"'  \n",
    "'CHASTON, JAMES \"JIM\"'  \n",
    "'BORDEN, DEBRA JO \"D-JO\"'  \n",
    "'ABRAHAM \"ABE\" HAMADEH', 'ABRAHAM HAMADEH'  \n",
    "  \n",
    "For values in party, 'WRITE IN DEM' should be 'DEM', 'LBT' should be 'LIB'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of rows where candidate is in 'UNDER VOTES', 'OVER VOTES', 'NOT QUALIFIED'\n",
    "exclude_candidates = ['UNDER VOTES', 'OVER VOTES', 'NOT QUALIFIED']\n",
    "election_results_cleaned = election_results_cleaned[~election_results_cleaned['candidate'].isin(exclude_candidates)]\n",
    "\n",
    "# replace candidate with 'WRITE-INS' with 'WRITE-IN'\n",
    "election_results_cleaned['candidate'] = election_results_cleaned['candidate'].replace('WRITE-INS', 'WRITE-IN')\n",
    "\n",
    "# Categorize duplicated parties\n",
    "election_results_cleaned['party'] = election_results_cleaned['party'].replace({\n",
    "    'WRITE IN DEM': 'DEM',\n",
    "    'LBT': 'LIB',\n",
    "})\n",
    "\n",
    "# Replace NA values in 'party' column with 'Other'\n",
    "election_results_cleaned['party'] = election_results_cleaned['party'].fillna('N')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use algorithms to find potential duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "candidates = ['ABRAHAM \"ABE\" HAMADEH', 'ABRAHAM HAMADEH', 'ADRIAN FONTES', 'AGUILAR, CESAR', 'ALAN SMITH', 'ALICE NOVOA', 'ALMA HERNANDEZ', 'ALSTON, LELA', 'ANDRES CANO', 'ANDY BIGGS', 'ANTHONY CAMBONI', 'AUSTIN, LORENA', 'BARBARA ROWLEY PARKER', 'BARRAZA, BRITTANI', 'BARTO, NANCY', 'BIASIUCCI, LEO', 'BIGGS, ANDY', 'BLAKE MASTERS', 'BLATTMAN, SETH', 'BORDEN, DEBRA JO \"D-JO\"', 'BORDES, SHERRISE', 'BORRELLI, SONNY', 'BRANNIES, MARYN M.', 'BRAVO, FLAVIO', 'BRIAN FERNANDEZ', 'BRIAN RADFORD', 'BULLOCK, CHRISTOPHER', 'BURCH, EVA', 'CAMBONI, ANTHONY', 'CARBONE, MICHAEL', 'CARROLL, FRANK', 'CARTER, NEAL', 'CASTEEN, JEANNE', 'CATHY RANSOM', 'CHAPLIK, JOSEPH', 'CHASTON, JAMES \"JIM\"', 'CHAVIRA CONTRERAS, LUPE', 'CHRIS MATHIS', 'CHRIS SARAPPO', 'CHRISTOPHER BULLOCK', 'CLARK, SANDA', 'CLEVELAND, JIM', 'CLINT WILLIAM SMITH', 'CONSUELO HERNANDEZ', 'CONTRERAS, PATRICIA \"PATTY\"', 'COOPER, KELLY', 'CORY MCGARR', 'CRANE, ELI', 'DAMIEN KENNEDY', 'DANA ALLMOND', 'DARROW, CADEN', 'DAVID CHRISTIAN FARNSWORTH', 'DAVID COOK', 'DAVID GOWAN', 'DAVID MARSHALL SR.', 'DAVIDA, EDWARD', 'DE LOS SANTOS, OSCAR', 'DEBORAH MCEWEN', 'DENNY, STEPH NOELLE \"STEPH\"', 'DI GENOVA, TRISTA', 'DI GENOVA, TRISTA \"TRISTA\"', 'DIAZ, LUPE', 'DUGGER, THOMAS', 'DUNN, TIMOTHY \"TIM\"', 'ELI CRANE', 'EPSTEIN, DENISE \"MITZI\"', 'FARNSWORTH, DAVID CHRISTIAN', 'FERNANDEZ, BRIAN', 'FIERRO, NICK', 'FINCHEM, MARK', 'FONTES, ADRIAN', 'GABALDON, ROSANNA', 'GABALDÓN, ROSANNA', 'GABALD√ÌN, ROSANNA', 'GAIL GRIFFIN', 'GALLEGO, RUBEN', 'GARCIA SNYDER, GARY', 'GARY GARCIA SNYDER', 'GILLETTE, JOHN', 'GOSAR, PAUL', 'GOWAN, DAVID', 'GRANTHAM, TRAVIS', 'GRAYSON, RICHARD', 'GRESS, MATT', 'GRIFFIN, GAIL', 'GRIJALVA, RAUL', 'GRIJALVA, RAUL 32', 'GRIJALVA, RAÚL', 'GRIJALVA, RA√ÖL', 'GRIJALVA, RA√ÖL 160', 'GUZMAN, ALIXANDRIA', 'HAMADEH, ABRAHAM \"ABE\"', 'HANS, CYNTHIA \"CINDY\"', 'HARRIS, LIZ', 'HEAP, JUSTIN', 'HELEN HUNTER', 'HENDRIX, LAURIN', 'HERNANDEZ, ANNA', 'HERNANDEZ, CONSUELO', 'HERNANDEZ, LYDIA', 'HERNANDEZ, MELODY', 'HOBBS, KATIE', 'HODGE, JEVIN D.', 'HOFFMAN, JAKE', 'HOLBROOK, STEPHANIE BLAIR', 'HOLZAPFEL, ROXANA', 'HUDELSON, ROB', 'HUNTER, HELEN', 'JACQUELINE PARKER', 'JACUELINE PARKER', 'JAKE HOFFMAN', 'JAVIER GARCIA RAMOS', 'JESUS LUGO JR.', 'JIM CLEVELAND', 'JOHN GILLETTE', 'JONES, STEPHAN \"STEVE\"', 'JUAN CISCOMANI', 'JUAN CISOMANI', 'JUSTIN HEAP', 'JUSTINE WADSACK', 'KAISER, STEVE', 'KARI LAKE', 'KATIE HOBBS', 'KAVANAGH, JOHN', 'KEITH SEAMAN', 'KELLY, MARK', 'KEN BENNETT', 'KENNEDY, DAMIEN', 'KENNEDY, SANDRA', 'KERBY, TAYLOR', 'KERN, ANTHONY', 'KERR, SINE', 'KEVIN THOMPSON', 'KIMBERLY YEE', 'KIRSTEN ENGEL', 'KISSINGER, DON', 'KOLODIN, ALEXANDER', 'KRIS MAYES', 'KRISTEN ENGEL', 'KUBY, LAUREN', 'KYLE NITSCHKE', 'LAKE, KARI', 'LAMAR, CHRISTIAN', 'LAUREN KUBY', 'LEO BIASIUCCI', 'LEO BISIUCCI', 'LESKO, DEBBIE', 'LESTER MAUL', 'LINDA EVANS', 'LIVINGSTON, DAVID', 'LONGDON, JENNIFER', 'LOUGHRIGE, BILL', 'LUGO, JESUS JR.', 'LUIS POZZOLO', 'LUPE DIAZ', 'LUTES-BURTON, MIKAELA SHONNIE  \"MIKKI\"', 'MAE PESHLAKAI', 'MARC J. VICTOR', 'MARIANA SANDOVAL', 'MARK FINCHEM', 'MARK KELLY', 'MARKY KELLY', 'MARSH, CHRISTINE', 'MARSH, PAUL', 'MARTIN QUEZADA', 'MARTINEZ, TERESA', 'MARTÍN QUEZADA', 'MART„N QUEZADA', 'MASTERS, BLAKE', 'MAUL, LESTER \"SKIP\"1', 'MAYES, KRIS', 'MCEWEN, DEBORAH', 'MCLEAN, TY RICHARD JR.', 'MENDEZ, JUAN', 'MENDOZA, MARY ANN', 'MERRILL, RAYSHAWN D\\'ANTHONY \"SHAWN\"', 'MESNARD, J.D.', 'MICHAEL CARBONE', 'MICHELE PENA', 'MIKAELA LUTES-BURTON', 'MIKE FOGEL', 'MIKE NICKERSON', 'MIRANDA, CATHERINE', 'MONTENEGRO, STEVE', 'MYERS, NICHOLAS \"NICK\"', 'MYRON TSOSIE', 'NANCY GUTIERREZ', 'NEAL CARTER', 'NEIL SINCLAIR', 'NICHOLAS \"NICK\" MYERS', 'NICK FIERRO', 'NICK MYERS', 'NOT QUALIFIED', 'NOVOA, ALICE', \"O'HALLERAN, TOM\", 'ORTIZ, ANALISE', 'OVER VOTES', 'PARKER, BARBARA ROWLEY', 'PARKER, JACQUELINE', 'PAUL GOSAR', 'PAUL MARSH', 'PAWLIK, JENNIFER', 'PAYNE, KEVIN', 'PEARCE, KATHY', 'PENA, MICHELE', 'PETERSEN, WARREN', 'PEÑA M., TATIANA', 'PINGERELLI, BEVERLY', 'PODEYN, SCOTT', 'POUNDS, WILLIAM JOSUÉ IV', 'POZZOLO, LUIS', 'PRIYA SUNDARESHAN', 'QUANG NGUYEN', 'QUEZADA, MARTIN', 'QUEZADA, MARTÍN', 'QUEZADA, MART√ÇN', 'QUINONEZ, MARCELINO', 'RACHEL JONES', 'RAMOS, JAVIER GARCIA', 'RAUL GRIJALVA', 'RAYMER, DAVID', 'RAYSHAWN MERRILL', 'REESE, BRANDY', 'RENEE ROXANNE', 'RICHARD TY MCLEAN, JR.', 'RICHARDSON, DAVID WAYNE', 'ROB HUDELSON', 'RODRIGUEZ, ROXANNE RENEE', 'ROE, TERRY', 'ROSANNA GABALDON', 'SALLY ANN GONZALES', 'SALMAN, ATHENA', 'SAMANTHA SEVERSON', 'SANDA CLARK', 'SANDOVAL, DAVID', 'SANDOVAL, MARIANA', 'SANDRA KENNEDY', 'SCANTLEBURY, ROBERT', 'SCHWEIKERT, DAVID', 'SCHWIEBERT, JUDY', 'SEAMAN, KEITH', 'SELINA BLISS', 'SEVERSON, SAMANTHA', 'SHAH, AMISH', 'SHAMP, JANAE', 'SHERRISE BORDES', 'SHOPE, THOMAS \"T.J.\"', 'SILVEY, JEFF', 'SINE KERR', 'SMELTZER, TODD JAMES', 'SMITH, ALAN', 'SMITH, AUSTIN', 'SMITH, CLINT WILLIAM', 'SONNY BORRELLI', 'SPREITZER, JEREMY', 'STAHL HAMILTON, STEPHANIE', 'STAN CAINE', 'STANTON, GREG', 'STEPH NOELLE DENNY', 'STEPHANIE STAHL HAMILTON', 'SUN, LEEZAH ELSA', 'SYMS, MARIA', 'T., TOM', 'TAYLOR KERBY', 'TAYLOR, WILLIAM MICHAEL \"WILL\"', 'TERECH, LAURA', 'TERESA MARTINEZ', 'TERÁN, RAQUEL', 'THERESA HATATHLIE', 'THOMAS \"T.J.\" SHOPE', 'THOMAS SHOPE', 'THOMPSON, KEVIN', 'TIMOTHY \"TIM\" DUNN', \"TOM O'HALLERAN\", 'TOMA, BEN', 'TRAVERS, ANASTASIA \"STACEY\"', 'TREADWELL, JENNIFER \"JENN\"', 'UNDER VOTES', 'VICTOR, MARC J.', 'WENDY ROGERS', 'WEST, LIANA', 'WILLIAM JOSUE POUNDS IV', 'WILLOUGHBY, JULIE', 'WILMETH, JUSTIN', 'WRITE-IN', 'WRITE-INS', 'YEE, KIMBERLY', 'ZINK, JEFF NELSON']\n",
    "\n",
    "# Function to normalize names by removing punctuation and lowercasing\n",
    "def normalize_name(name):\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n",
    "    return name.lower().strip()  # Convert to lowercase and strip whitespace\n",
    "# Normalize all names\n",
    "normalized_candidates = [normalize_name(name) for name in candidates]\n",
    "\n",
    "# Dictionary to store potential duplicates\n",
    "potential_duplicates = defaultdict(list)\n",
    "\n",
    "# Use fuzzy matching to identify potential duplicates within the list\n",
    "for i, name in enumerate(normalized_candidates):\n",
    "    matches = process.extract(name, normalized_candidates, limit=len(normalized_candidates))\n",
    "    for match in matches:\n",
    "        # Ensure match isn't the same name and check similarity score\n",
    "        if match[1] > 85 and match[0] != name:\n",
    "            potential_duplicates[name].append((candidates[i], match))\n",
    "\n",
    "# Print potential duplicates\n",
    "print(\"Potential duplicates:\")\n",
    "duplicates_found = False\n",
    "for normalized_name, matches in potential_duplicates.items():\n",
    "    if len(matches) > 1:  # More than one match indicates potential duplicates\n",
    "        duplicates_found = True\n",
    "        print(f\"Normalized Name: {normalized_name}\")\n",
    "        for original, (match_name, score) in matches:\n",
    "            print(f\"  - Original: {original} is similar to: {candidates[normalized_candidates.index(match_name)]} with score {score}\")\n",
    "\n",
    "if not duplicates_found:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping old candidate names to new candidate names\n",
    "replacement_dict = {\n",
    "    'ABRAHAM \"ABE\" HAMADEH': 'ABRAHAM HAMADEH',\n",
    "    'HAMADEH, ABRAHAM \"ABE\"': 'ABRAHAM HAMADEH',\n",
    "    'BORDEN, DEBRA JO \"D-JO\"': 'DEBRA JO BORDEN',\n",
    "    'CHASTON, JAMES \"JIM\"': 'CHASTON, JAMES',\n",
    "    'DENNY, STEPH NOELLE \"STEPH\"': 'DENNY, STEPH NOELLE',\n",
    "    'DUNN, TIMOTHY \"TIM\"': 'DUNN TIMOTHY',\n",
    "    'EPSTEIN, DENISE \"MITZI\"': 'EPSTEIN, DENISE',\n",
    "    'GABALDON, ROSANNA': 'GABALDÓN, ROSANNA',\n",
    "    'GARCIA SNYDER, GARY': 'GARY, GARCIA SNYDER',\n",
    "    'GRIJALVA, RAUL 32': 'RAUL GRIJALVA',\n",
    "    'GRIJALVA, RAÚL': 'RAUL GRIJALVA',\n",
    "    'GRIJALVA, RAÖL': 'RAUL GRIJALVA',\n",
    "    'GRIJALVA, RAÖL 160': 'RAUL GRIJALVA',\n",
    "    'HANS, CYNTHIA \"CINDY\"': 'HANS, CYNTHIA',\n",
    "    'JACUELINE PARKER': 'JACQUELINE PARKER',\n",
    "    'LUTES-BURTON, MIKAELA SHONNIE \"MIKKI\"': 'MIKARLA LUTES-BURTON', \n",
    "    'MARKY KELLY': 'MARK KELLY',\n",
    "    'MART„N QUEZADA': 'MARTÍN QUEZADA',\n",
    "    'QUEZADA, MART√ÇN':'MARTÍN QUEZADA',\n",
    "    'MERRILL, RAYSHAWN D\\'ANTHONY \"SHAWN\"': 'RAYSHAWN, MERRILL',\n",
    "    'MAUL, LESTER \"SKIP\"1': 'MAUL, LESTER',\n",
    "    'BRANNIES, MARYN M.': 'MARYN, BRANNIES',\n",
    "    'CHAVIRA CONTRERAS, LUPE': 'LUPE, CONTRERAS',\n",
    "    'CONTRERAS, PATRICIA \"PATTY\"': 'PATRICIA CONTRERAS',\n",
    "    'DI GENOVA, TRISTA \"TRISTA\"': 'DI GENOVA, TRISTA',\n",
    "    'GABALD√ÌN, ROSANNA': 'GABALDÓN, ROSANNA',\n",
    "    'GARY, GARCIA SNYDER': 'GARY GARCIA SNYDER',\n",
    "    'GRIJALVA, RA√ÖL': 'RAUL GRIJALVA',\n",
    "    'GRIJALVA, RA√ÖL 160': 'RAUL GRIJALVA',\n",
    "    'GRIJALVA, RAUL': 'RAUL GRIJALVA',\n",
    "    'HODGE, JEVIN D.': 'JEVIN, HODGE',\n",
    "    'JONES, STEPHAN \"STEVE\"': 'JONES STEPHAN',\n",
    "    'JUAN CISOMANI': 'JUAN CISCOMANI',\n",
    "    'LEO BISIUCCI': 'LEO BIASIUCCI',\n",
    "    'MARTIN QUEZADA': 'MARTÍN QUEZADA',\n",
    "    'QUEZADA, MARTIN': 'MARTÍN QUEZADA',\n",
    "    'QUEZADA, MARTÍN': 'MARTÍN QUEZADA',\n",
    "    'FONTES, ADRIAN': 'ADRIAN FONTES',\n",
    "    'AGUILAR, CESAR': 'CESAR AGUILAR',\n",
    "    'SMITH, ALAN': 'ALAN SMITH',\n",
    "    'NOVOA, ALICE': 'ALICE NOVOA',\n",
    "    'ALSTON, LELA': 'LELA ALSTON',\n",
    "    'BIGGS, ANDY': 'ANDY BIGGS',\n",
    "    'CAMBONI, ANTHONY': 'ANTHONY CAMBONI',\n",
    "    'AUSTIN, LORENA': 'LORENA AUSTIN',\n",
    "    'BARBARA ROWLEY PARKER': 'BARBARA PARKER',\n",
    "    'PARKER, BARBARA ROWLEY': 'BARBARA PARKER',\n",
    "    'BARRAZA, BRITTANI': 'BRITTANI, BARRAZA',\n",
    "    'BARTO, NANCY': 'NANCY BARTO',\n",
    "    'BIASIUCCI, LEO': 'LEO BIASIUCCI',\n",
    "    'MASTERS, BLAKE': 'BLAKE MASTERS',\n",
    "    'BLATTMAN, SETH': 'SETH BLATTMAN',\n",
    "    'BORDES, SHERRISE': 'SHERRISE BORDES',\n",
    "    'BORRELLI, SONNY': 'SONNY BORRELLI',\n",
    "    'BRAVO, FLAVIO': 'FLAVIO BRAVO',\n",
    "    'FERNANDEZ, BRIAN': 'BRIAN FERNANDEZ',\n",
    "    'BULLOCK, CHRISTOPHER': 'CHRISTOPHER BULLOCK',\n",
    "    'BURCH, EVA': 'EVA BURCH',\n",
    "    'CARBONE, MICHAEL': 'MICHAEL CARBONE',\n",
    "    'CARROLL, FRANK': 'FRANK CARROLL',\n",
    "    'CARTER, NEAL': 'NEAL CARTER',\n",
    "    'CASTEEN, JEANNE': 'JEANNE CASTEEN',\n",
    "    'JESUS JR. LUGO': 'JESUS LUGO',\n",
    "    'MIKAELA SHONNIE  \"MIKKI\" LUTES-BURTON': 'MIKAELA LUTES-BURTON',\n",
    "    'NICHOLAS \"NICK\" MYERS': 'NICK MYERS',\n",
    "    'ROSANNA GABALDON': 'ROSANNA GABALDÓN', \n",
    "    'TATIANA PEÑA M.': 'TATIANA PEÑA',\n",
    "    'THOMAS \"T.J.\" SHOPE': 'THOMAS SHOPE',\n",
    "    'TY RICHARD JR. MCLEAN': 'TY MCLEAN',\n",
    "    'WILLIAM JOSUE POUNDS IV': 'WILLIAM POUNDS',\n",
    "    'WILLIAM JOSUÉ IV POUNDS': 'WILLIAM POUNDS',\n",
    "    'RA√ÖL GRIJALVA': 'RAUL GRIJALVA',\n",
    "    'ROSANNA GABALD√ÌN': 'ROSANNA GABALDÓN',\n",
    "    'RAÚL GRIJALVA': 'RAUL GRIJALVA',\n",
    "    'RA√ÖL 160 GRIJALVA': 'RAUL GRIJALVA',\n",
    "    'RA√ÖL GRIJALVA': 'RAUL GRIJALVA',\n",
    "    'RAUL 32 GRIJALVA': 'RAUL GRIJALVA',\n",
    "    'STEPH NOELLE \"STEPH\" DENNY': 'STEPH NOELLE DENNY',\n",
    "    'MART√ÇN QUEZADA': 'MARTÍN QUEZADA',\n",
    "    'JR. RICHARD TY MCLEAN': 'TY MCLEAN',\n",
    "    'RAYSHAWN D\\'ANTHONY \"SHAWN\" MERRILL': 'RAYSHAWN MERRILL',\n",
    "    'KRISTEN ENGEL': 'KIRSTEN ENGEL',\n",
    "    'TOM T.': 'WRITE-IN',\n",
    "    'WRITE_IN': 'WRITE-IN',\n",
    "}\n",
    "\n",
    "# Replace old candidate names with new names\n",
    "election_results_cleaned['candidate'] = election_results_cleaned['candidate'].replace(replacement_dict)\n",
    "\n",
    "unique_candidates_after_mapping = sorted(election_results_cleaned['candidate'].unique())\n",
    "unique_candidates_after_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "everything now looks right, besides this guy 'TOM T.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where candidate is 'TOM T.'\n",
    "tom_t_rows = election_results_cleaned[election_results_cleaned['candidate'] == 'TOM T.']\n",
    "\n",
    "tom_t_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After searching, there is no such person as Tom T and he had no party affiliation, only Tom O'Halleran who runs for U.S. Representative in Congress - District No. 2. So change this Tom T to write-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the columns in the final DataFrame\n",
    "print(\"Column names in the final DataFrame:\")\n",
    "print(final_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by office, district, and candidate, and sum the votes\n",
    "office_district_candidate_votes = election_results_cleaned.groupby(['office', 'district', 'candidate'])['votes'].sum()\n",
    "\n",
    "# Print the number of votes for each candidate\n",
    "for (office, district, candidate), votes in office_district_candidate_votes.items():\n",
    "    district_info = f\"District {int(district)}\" if pd.notna(district) else \"At Large\"\n",
    "    print(f\"Office: {office} - {district_info} - Candidate: {candidate}, Votes: {votes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Election Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_results_cleaned.columns = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for 'office'\n",
    "office_mapping = {\n",
    "    'ATTORNEY GENERAL': 'ATG',\n",
    "    'CORPORATION COMMISSIONER': 'COC',\n",
    "    'GOVERNOR': 'GOV',\n",
    "    'SECRETARY OF STATE': 'SOS',\n",
    "    'STATE HOUSE': 'SL',\n",
    "    'STATE MINE INSPECTOR': 'STM',\n",
    "    'STATE SENATE': 'SU',\n",
    "    'STATE TREASURER': 'STT',\n",
    "    'U.S. HOUSE': 'USH',\n",
    "    'U.S. SENATE': 'USS'\n",
    "}\n",
    "\n",
    "\n",
    "# Find the largest district number\n",
    "max_district = int(election_results_cleaned['district'].max())\n",
    "\n",
    "# Determine the number of digits for district formatting\n",
    "num_digits = len(str(max_district))\n",
    "\n",
    "# Function to create a unique column name and print its meaning\n",
    "def create_column_name(row, existing_names, num_digits):\n",
    "    office_abbr = office_mapping.get(row['office'], '')\n",
    "    party_initial = row['party'][0]\n",
    "    last_name = row['candidate'].split()[-1][:3].upper()\n",
    "    \n",
    "    if pd.notna(row['district']):\n",
    "        district = str(int(row['district'])).zfill(num_digits)  # Ensure district has leading zeros\n",
    "        base_name = f\"G{office_abbr}{district}{party_initial}{last_name}\"\n",
    "    else:\n",
    "        base_name = f\"G22{office_abbr}{party_initial}{last_name}\"\n",
    "    \n",
    "    # Ensure the column name is unique\n",
    "    if base_name not in existing_names:\n",
    "        unique_name = base_name\n",
    "    else:\n",
    "        suffix = 1\n",
    "        unique_name = f\"{base_name}_{suffix}\"\n",
    "        while unique_name in existing_names:\n",
    "            suffix += 1\n",
    "            unique_name = f\"{base_name}_{suffix}\"\n",
    "    \n",
    "    # Print the meaning of the generated column name\n",
    "    district_info = f\"District {district}\" if pd.notna(row['district']) else \"General Election Year 2022\"\n",
    "    print(f\"{unique_name}: Office - {row['office']}, {district_info}, Party - {row['party']}, Candidate - {row['candidate']}\")\n",
    "\n",
    "    return unique_name\n",
    "\n",
    "\n",
    "# Get unique combinations of counties and precincts\n",
    "unique_combinations = election_results_cleaned[['county', 'precinct']].drop_duplicates()\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over each unique combination of county and precinct\n",
    "for _, unique_row in unique_combinations.iterrows():\n",
    "    county = unique_row['county']\n",
    "    precinct = unique_row['precinct']\n",
    "    \n",
    "    # Filter for the specific county and precinct\n",
    "    filtered_df = election_results_cleaned[(election_results_cleaned['county'] == county) & (election_results_cleaned['precinct'] == precinct)]\n",
    "    \n",
    "    # Track existing column names to ensure uniqueness\n",
    "    existing_names = set()\n",
    "    \n",
    "    # Apply the function to create new column names\n",
    "    filtered_df['ColumnName'] = filtered_df.apply(lambda row: create_column_name(row, existing_names, num_digits), axis=1)\n",
    "    \n",
    "    # Update the set of existing names\n",
    "    existing_names.update(filtered_df['ColumnName'].tolist())\n",
    "    \n",
    "    # Pivot dataset so that each row represents a precinct and each column represents a candidate\n",
    "    pivoted_df = filtered_df.pivot_table(\n",
    "        index=['county', 'precinct'],\n",
    "        columns='ColumnName',\n",
    "        values='votes',\n",
    "        aggfunc='sum'\n",
    "    )\n",
    "    \n",
    "    # Flatten the columns\n",
    "    pivoted_df.columns = [col for col in pivoted_df.columns]\n",
    "    \n",
    "    # Reset index\n",
    "    pivoted_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Append results\n",
    "    results.append(pivoted_df)\n",
    "\n",
    "# Concatenate all results\n",
    "final_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of precincts for each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unique_df1 = final_df.groupby('county')['precinct'].nunique()\n",
    "for county, count in count_unique_df1.items():\n",
    "    print(f\"{county}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read AZ 2020 shapefile for potential re-use, in case where 2022 data is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path = '../az_gen_20_prec/az_gen_20_prec.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Select only the desired columns\n",
    "selected_columns = ['geometry', 'PRECINCTNA', 'COUNTY_NAM', 'UNIQUE_ID']\n",
    "gdf_2020 = gdf[selected_columns]\n",
    "\n",
    "def remove_special_characters(column):\n",
    "    return column.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "gdf_2020['PRECINCTNA'] = remove_special_characters(gdf_2020['PRECINCTNA'])\n",
    "gdf_2020['COUNTY_NAM'] = remove_special_characters(gdf_2020['COUNTY_NAM'])\n",
    "gdf_2020['UNIQUE_ID'] = remove_special_characters(gdf_2020['UNIQUE_ID'])\n",
    "\n",
    "print(gdf_2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process precinct boundaries by county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precincts_with_labels(gdf):\n",
    "    \"\"\"\n",
    "    Plots a GeoDataFrame with color-coded precincts and labels, using parameters to customize the column for color, labels, figure size, colormap, edge color, line width, and label font size.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(40, 40))\n",
    "    gdf.plot(column='precinct', ax=ax, legend=True, cmap='tab20', edgecolor='black', linewidth=0.5)\n",
    "    for x, y, label in zip(gdf.geometry.centroid.x, gdf.geometry.centroid.y, gdf['precinct']):\n",
    "        ax.text(x, y, label, fontsize=15, ha='center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precincts(gdf, col_name):\n",
    "    \"\"\"\n",
    "    Plots a GeoDataFrame with color-coded precincts and labels, using parameters to customize the column for color, labels, figure size, colormap, edge color, line width, and label font size.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(40, 40))\n",
    "    gdf.plot(column=col_name, ax=ax, legend=True, cmap='tab20', edgecolor='black', linewidth=0.5)\n",
    "    for x, y, label in zip(gdf.geometry.centroid.x, gdf.geometry.centroid.y, gdf[col_name]):\n",
    "        ax.text(x, y, label, fontsize=15, ha='center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coconino County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows \n",
    "coconino_rows = final_df[final_df['county'].str.lower() == 'coconino']\n",
    "\n",
    "# Display the filtered rows\n",
    "print(coconino_rows.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the GeoJSON file\n",
    "geojson_file_Cococino = './precinct_boundaries/Cococino.geojson'\n",
    "gdf_Cococino = gpd.read_file(geojson_file_Cococino)\n",
    "\n",
    "print(len(gdf_Cococino))\n",
    "\n",
    "final_df_Coconino = coconino_rows\n",
    "\n",
    "# Extract the numerical part from the precinct values\n",
    "final_df_Coconino['precinct'] = final_df_Coconino['precinct'].str.extract('(\\d+)')[0].astype(int)\n",
    "\n",
    "gdf_Cococino['VOTENUM'] = gdf_Cococino['VOTENUM'].astype(int)\n",
    "\n",
    "\n",
    "# Merge the DataFrames on precinct_num and VOTENUM\n",
    "merged_df = final_df_Coconino.merge(gdf_Cococino[['VOTENUM', 'geometry']], left_on='precinct', right_on='VOTENUM', how='right')\n",
    "\n",
    "# Convert final_df to a GeoDataFrame\n",
    "gdf_Cococino_final = gpd.GeoDataFrame(merged_df, geometry='geometry')\n",
    "final_df_Coconino['precinct'] = final_df_Coconino['precinct'].astype(str)\n",
    "\n",
    "plot_precincts_with_labels(gdf_Cococino_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_Cococino_final.drop(columns=['VOTENUM'], inplace=True)\n",
    "gdf_Cococino_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache County\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_2020 = gdf_2020[gdf_2020['COUNTY_NAM'] == 'Apache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_rows = final_df[final_df['county'].str.lower() == 'apache']\n",
    "apache_rows['precinct'] = apache_rows['precinct'].str.upper()\n",
    "print(apache_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on UNIQUE_ID and precinct\n",
    "gdf_Apache_final = apache_rows.merge(apache_2020[['UNIQUE_ID', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='UNIQUE_ID', \n",
    "                                       how='right')\n",
    "\n",
    "gdf_Apache_final = gdf_Apache_final.drop(columns=['UNIQUE_ID'])\n",
    "\n",
    "gdf_Apache_final = gpd.GeoDataFrame(gdf_Apache_final, geometry='geometry')\n",
    "\n",
    "\n",
    "plot_precincts_with_labels(gdf_Apache_final)\n",
    "gdf_Apache_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cochise County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cochise_rows = final_df[final_df['county'].str.lower() == 'cochise']\n",
    "cochise_rows['precinct'] = remove_special_characters(cochise_rows['precinct'])\n",
    "cochise_rows['precinct'] = cochise_rows['precinct'].str.extract('(\\d+)')[0].astype(str).str.lstrip('0').astype(int)\n",
    "print(cochise_rows.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path_Cochise = './precinct_boundaries/Chochise/Election_Precinct_2022.shp'\n",
    "gdf_Cochise = gpd.read_file(shapefile_path_Cochise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "gdf_Cochise_final = cochise_rows.merge(gdf_Cochise[['prct_num', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='prct_num', \n",
    "                                       how='right')\n",
    "\n",
    "# Drop the duplicate col\n",
    "gdf_Cochise_final = gdf_Cochise_final.drop(columns=['prct_num'])\n",
    "gdf_Cochise_final = gpd.GeoDataFrame(gdf_Cochise_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_Cochise_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gila County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gila_rows = final_df[final_df['county'].str.lower() == 'gila']\n",
    "gila_rows['precinct'] = remove_special_characters(gila_rows['precinct'])\n",
    "def clean_precinct(precinct):\n",
    "    precinct = precinct.replace(' ', '')\n",
    "    precinct = precinct[:-3]\n",
    "    return precinct\n",
    "\n",
    "gila_rows['precinct'] = gila_rows['precinct'].apply(lambda x: clean_precinct(x))\n",
    "sorted(gila_rows['precinct'])\n",
    "\n",
    "mapping = {\n",
    "   'MiamiNo1': 'Miami1',\n",
    "   'MiamiNo3': 'Miami3'\n",
    "}\n",
    "\n",
    "# Replace old candidate names with new names\n",
    "gila_rows['precinct'] = gila_rows['precinct'].replace(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path_Gila = './precinct_boundaries/Gela/GilaPrecincts_Dec2022.shp'\n",
    "gdf_Gila = gpd.read_file(shapefile_path_Gila)\n",
    "gdf_Gila['NAME20'] = remove_special_characters(gdf_Gila['NAME20'])\n",
    "gdf_Gila['NAME20'] = gdf_Gila['NAME20'].str.replace(' ', '')\n",
    "gdf_Gila['NAME20']\n",
    "plot_precincts(gdf_Gila,'NAME20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "gdf_Gila_final = gila_rows.merge(gdf_Gila[['NAME20', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='NAME20', \n",
    "                                       how='right')\n",
    "\n",
    "# Drop the duplicate col\n",
    "gdf_Gila_final = gdf_Gila_final.drop(columns=['NAME20'])\n",
    "gdf_Gila_final = gpd.GeoDataFrame(gdf_Gila_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_Gila_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graham County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_file_Graham = './precinct_boundaries/Graham.geojson'\n",
    "gdf_Graham = gpd.read_file(geojson_file_Graham)\n",
    "if len(gdf_Graham) != 22:\n",
    "    raise ValueError(\"The GeoDataFrame must have exactly 22 rows.\")\n",
    "\n",
    "# Assign values from 1 to 22 to the 'name' column\n",
    "gdf_Graham['NAME'] = range(1, 23)\n",
    "print(gdf_Graham)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graham_rows = final_df[final_df['county'].str.lower() == 'graham']\n",
    "graham_rows['precinct'] = graham_rows['precinct'].str.extract('(\\d+)')[0].astype(int)\n",
    "print(graham_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "gdf_Graham_final = graham_rows.merge(gdf_Graham[['NAME', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='NAME', \n",
    "                                       how='right')\n",
    "\n",
    "# Drop the duplicate col\n",
    "gdf_Graham_final = gdf_Graham_final.drop(columns=['NAME'])\n",
    "\n",
    "gdf_Graham_final = gpd.GeoDataFrame(gdf_Graham_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_Graham_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greenlee County\n",
    "Using 2020 boundaries(2022 boundaries provided are school districts not precinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenlee_rows = final_df[final_df['county'].str.lower() == 'greenlee']\n",
    "greenlee_rows['precinct'] = greenlee_rows['precinct'].str[4:].str.upper()\n",
    "greenlee_rows['precinct'] = remove_special_characters(greenlee_rows['precinct'])\n",
    "\n",
    "print(greenlee_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenlee_2020 = gdf_2020[gdf_2020['COUNTY_NAM'] == 'Greenlee']\n",
    "print(greenlee_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "gdf_Greenlee_final = greenlee_rows.merge(greenlee_2020[['PRECINCTNA', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='PRECINCTNA', \n",
    "                                       how='right')\n",
    "\n",
    "# Drop the duplicate col\n",
    "gdf_Greenlee_final = gdf_Greenlee_final.drop(columns=['PRECINCTNA'])\n",
    "\n",
    "gdf_Greenlee_final = gpd.GeoDataFrame(gdf_Greenlee_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_Greenlee_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La Paz County\n",
    "Using 2020 boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_paz_rows = final_df[final_df['county'].str.lower() == 'la paz']\n",
    "la_paz_rows['precinct'] = la_paz_rows['precinct'].str.upper()\n",
    "\n",
    "print(la_paz_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_paz_2020 = gdf_2020[gdf_2020['COUNTY_NAM'] == 'La Paz']\n",
    "print(la_paz_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "gdf_La_Paz_final = la_paz_rows.merge(la_paz_2020[['UNIQUE_ID', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='UNIQUE_ID', \n",
    "                                       how='left')\n",
    "\n",
    "# Drop the duplicate col\n",
    "gdf_La_Paz_final = gdf_La_Paz_final.drop(columns=['UNIQUE_ID'])\n",
    "\n",
    "gdf_La_Paz_final = gpd.GeoDataFrame(gdf_La_Paz_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_La_Paz_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maricopa County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_file_Maricopa = './precinct_boundaries/Maricopa.geojson'\n",
    "gdf_Maricopa = gpd.read_file(geojson_file_Maricopa)\n",
    "\n",
    "print(sorted(gdf_Maricopa['PctNum']))\n",
    "print(len(gdf_Maricopa['PctNum']))\n",
    "plot_precincts(gdf_Maricopa, 'PctNum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maricopa_rows = final_df[final_df['county'].str.lower() == 'maricopa']\n",
    "maricopa_rows['precinct'] = maricopa_rows['precinct'].str.lower().str[1:4].str.lstrip('0')\n",
    "print(len(maricopa_rows['precinct']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The election results shows that Maricopa has 904 precincts, but precincts boyndaries shows that it has 935 precincts.\n",
    "The official website mentioned that there are some precincts with no voters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_Maricopa['PctNum'] = gdf_Maricopa['PctNum'].astype(str)\n",
    "maricopa_rows['precinct'] = maricopa_rows['precinct'].astype(str)\n",
    "\n",
    "gdf_Maricopa_final = pd.merge(maricopa_rows, gdf_Maricopa[['PctNum', 'geometry']], how='right', left_on='precinct', right_on='PctNum')\n",
    "\n",
    "# Reindex to ensure all columns from maricopa_rows are included, with geometry column added\n",
    "gdf_Maricopa_final = gdf_Maricopa_final.reindex(columns=list(maricopa_rows.columns) + ['geometry'])\n",
    "\n",
    "\n",
    "gdf_Maricopa_final = gpd.GeoDataFrame(gdf_Maricopa_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_Maricopa_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mohave County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path_Mohave = './precinct_boundaries/Mohave/Voting_Precincts_2023.shp'\n",
    "gdf_Mohave = gpd.read_file(shapefile_path_Mohave)\n",
    "\n",
    "print(gdf_Mohave)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mohave_rows = final_df[final_df['county'].str.lower() == 'mohave']\n",
    "mohave_rows['precinct'] = mohave_rows['precinct'].str[4:].str.upper()\n",
    "mohave_rows['precinct'] = remove_special_characters(mohave_rows['precinct'])\n",
    "print(mohave_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_Mohave_final = mohave_rows.merge(gdf_Mohave[['NAME', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='NAME', \n",
    "                                       how='right')\n",
    "\n",
    "gdf_Mohave_final = gdf_Mohave_final.drop(columns=['NAME'])\n",
    "\n",
    "gdf_Mohave_final = gpd.GeoDataFrame(gdf_Mohave_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_Mohave_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navajo County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navajo_rows = final_df[final_df['county'].str.lower() == 'navajo']\n",
    "print(navajo_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navajo_2020 = gdf_2020[gdf_2020['COUNTY_NAM'] == 'Navajo']\n",
    "print(navajo_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both columns to uppercase\n",
    "navajo_rows['precinct'] = navajo_rows['precinct'].str.upper()\n",
    "navajo_2020['UNIQUE_ID'] = navajo_2020['UNIQUE_ID'].str.upper()\n",
    "\n",
    "# Check for matching values again\n",
    "print(navajo_rows['precinct'].unique())\n",
    "print(navajo_2020['UNIQUE_ID'].unique())\n",
    "\n",
    "# Merge the dataframes on UNIQUE_ID and precinct\n",
    "gdf_Navajo_final = navajo_rows.merge(navajo_2020[['UNIQUE_ID', 'geometry']], \n",
    "                                     left_on='precinct', \n",
    "                                     right_on='UNIQUE_ID', \n",
    "                                     how='right')\n",
    "\n",
    "# Drop the duplicate UNIQUE_ID column from the merged DataFrame\n",
    "gdf_Navajo_final = gdf_Navajo_final.drop(columns=['UNIQUE_ID'])\n",
    "gdf_Navajo_final = gpd.GeoDataFrame(gdf_Navajo_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_Navajo_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pima County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path_Pima = './precinct_boundaries/Pima/Districts_-_Voter_Precincts.shp'\n",
    "gdf_Pima = gpd.read_file(shapefile_path_Pima)\n",
    "print(gdf_Pima)\n",
    "plot_precincts(gdf_Pima, 'OBJECTID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_rows = final_df[final_df['county'].str.lower() == 'pima']\n",
    "print(pima_rows)\n",
    "print(len(pima_rows['precinct'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official website mentioned that \"2/28/22 - Elections proposed many boundary adjustments, including the creation of new precincts, in order to align with the new Congressional and Legislative District boundaries created by the Arizona Independent Redistricting Commission. Number of precincts increased from 249 to 278. Board of Supervisors approved changes on 2/15/2022\". However, the election result only has 266 precints from both sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'precinct' column in pinal_rows to string\n",
    "pima_rows['precinct'] = pima_rows['precinct'].astype(str)\n",
    "\n",
    "# Convert the 'PRECINCT' column in gdf_Pima to string\n",
    "gdf_Pima['PRECINCT'] = gdf_Pima['PRECINCT'].astype(str)\n",
    "\n",
    "# Perform the left join merge\n",
    "gdf_Pima_final = pima_rows.merge(gdf_Pima[['PRECINCT', 'geometry']], \n",
    "                                  left_on='precinct', \n",
    "                                  right_on='PRECINCT', \n",
    "                                  how='right')\n",
    "\n",
    "# Drop the duplicate 'PRECINCT' column from the merged DataFrame\n",
    "gdf_Pima_final = gdf_Pima_final.drop(columns=['PRECINCT'])\n",
    "\n",
    "gdf_Pima_final = gpd.GeoDataFrame(gdf_Pima_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_Pima_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinal County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path_Pinal = './precinct_boundaries/Pinal/Voter_Precincts.shp'\n",
    "gdf_Pinal = gpd.read_file(shapefile_path_Pinal)\n",
    "print(gdf_Pinal)\n",
    "plot_precincts(gdf_Pinal, 'NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values in the 'precinct' column\n",
    "unique_name = gdf_Pinal['NAME'].unique()\n",
    "\n",
    "# Sort the unique values in alphabetical order\n",
    "unique_name_sorted = sorted(unique_name)\n",
    "print(len(unique_name_sorted))\n",
    "\n",
    "# Print each unique value line by line\n",
    "for precinct in unique_name_sorted:\n",
    "    print(precinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinal_rows = final_df[final_df['county'].str.lower() == 'pinal']\n",
    "pinal_rows['precinct'] = pinal_rows['precinct'].str[3:].str.upper()\n",
    "print(pinal_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 precincts that do not match, 4 of them are minor name differences and 1 is no vote precint. eg. Apache Junction Shouth -> Apache JCT South. And SAN CARLOS COMMUNITY in the shapefile does not match with any election precincts, which I still include by filled it with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_mapping = {\n",
    "    'APACHE JCT SOUTH': 'APACHE JUNCTION SOUTH',\n",
    "    'APACHE JCT SUPERSTITION': 'APACHE JUNCTION SUPERSTITION',\n",
    "    'COYOTE': 'COYOTE RANCH',\n",
    "    'SADDLEBROOKE RANCHE': 'SADDLEBROOKE RANCH'\n",
    "}\n",
    "\n",
    "pinal_rows['precinct'] = pinal_rows['precinct'].replace(value_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinal_rows['precinct'] = pinal_rows['precinct'].str.strip()\n",
    "unique_precincts = pinal_rows['precinct'].unique()\n",
    "\n",
    "# Sort the unique values in alphabetical order\n",
    "unique_precincts_sorted = sorted(unique_precincts)\n",
    "print(len(unique_precincts_sorted))\n",
    "\n",
    "# Print each unique value line by line\n",
    "for precinct in unique_precincts_sorted:\n",
    "    print(precinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the right merge to keep all geometries from gdf_Pinal\n",
    "gdf_Pinal_final = pinal_rows.merge(\n",
    "    gdf_Pinal[['NAME', 'geometry']], \n",
    "    left_on='precinct', \n",
    "    right_on='NAME', \n",
    "    how='right'\n",
    ")\n",
    "\n",
    "# # Drop the 'NAME' column to avoid duplication and confusion\n",
    "# gdf_Pinal_final = gdf_Pinal_final.drop(columns=['NAME'])\n",
    "\n",
    "# Convert the merged DataFrame to a GeoDataFrame\n",
    "gdf_Pinal_final = gpd.GeoDataFrame(gdf_Pinal_final, geometry='geometry')\n",
    "\n",
    "\n",
    "# Plot the precincts with labels\n",
    "plot_precincts(gdf_Pinal_final, 'NAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Santa Cruz County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_cruz_2020 = gdf_2020[gdf_2020['COUNTY_NAM'] == 'Santa Cruz']\n",
    "print(santa_cruz_2020)\n",
    "print(len(santa_cruz_2020))\n",
    "print(sorted(santa_cruz_2020['UNIQUE_ID']))\n",
    "plot_precincts(santa_cruz_2020, 'UNIQUE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_cruz_rows = final_df[final_df['county'].str.lower() == 'santa cruz']\n",
    "# print(santa_cruz_rows)\n",
    "\n",
    "# After checking multiple sources, the election result marked Calabas as 16 and 17 but instead there is only 1 Calabas precinct. So drop it:\n",
    "santa_cruz_rows = santa_cruz_rows[santa_cruz_rows['precinct'] != 'Calabasas 16']\n",
    "\n",
    "# Also, Beca 19.6 should be Beca 19 19.6 for consistency\n",
    "santa_cruz_rows.loc[santa_cruz_rows['precinct'] == 'Baca 19.6', 'precinct'] = 'Baca 19 19.6'\n",
    "\n",
    "# Function to determine the simplified name for grouping\n",
    "def get_simplified_name(precinct):\n",
    "    parts = precinct.split()\n",
    "    if len(parts) > 2:\n",
    "        if any(char.isdigit() for char in parts[2]):\n",
    "            return ' '.join(parts[:2])\n",
    "    return precinct\n",
    "    \n",
    "# Create a new column with the simplified name for grouping\n",
    "santa_cruz_rows['simplified_precinct'] = santa_cruz_rows['precinct'].apply(get_simplified_name)\n",
    "\n",
    "# Define aggregation functions for all columns\n",
    "aggregation_functions = {col: 'first' for col in santa_cruz_rows.columns if col != 'simplified_precinct'}\n",
    "\n",
    "# Group by the simplified name and aggregate\n",
    "grouped_df = santa_cruz_rows.groupby('simplified_precinct').agg(aggregation_functions).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def simplify_precinct_grouped(precinct):\n",
    "    parts = precinct.split()\n",
    "    if len(parts) > 1:\n",
    "        return ' '.join(parts[:2])\n",
    "    return precinct\n",
    "\n",
    "# Apply the function to the 'precinct' column\n",
    "grouped_df['precinct'] = grouped_df['precinct'].apply(simplify_precinct_grouped)\n",
    "\n",
    "mapping = {\n",
    "    'Peck Canyon': 'Peck Canyon 21', \n",
    "    'Rio Rico': 'Rio Rico 7',\n",
    "    'Santa Cruz': 'Santa Cruz 13',\n",
    "    'Lake Patagonia': 'Lake Patagonia 24',\n",
    "}\n",
    "grouped_df['precinct'] = grouped_df['precinct'].replace(mapping)\n",
    "\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(len(grouped_df))\n",
    "sorted(grouped_df['precinct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on UNIQUE_ID and precinct\n",
    "gdf_Santa_Cruz_final = grouped_df.merge(santa_cruz_2020[['PRECINCTNA', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='PRECINCTNA', \n",
    "                                       how='right')\n",
    "\n",
    "# Drop the duplicate UNIQUE_ID column from the merged DataFrame\n",
    "gdf_Santa_Cruz_final = gdf_Santa_Cruz_final.drop(columns=['PRECINCTNA'])\n",
    "\n",
    "\n",
    "gdf_Santa_Cruz_final = gpd.GeoDataFrame(gdf_Santa_Cruz_final, geometry='geometry')\n",
    "\n",
    "plot_precincts_with_labels(gdf_Santa_Cruz_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yavapai County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path_Yavapai = './precinct_boundaries/Yavapai/YC_ElectionPrecincts.shp'\n",
    "gdf_Yavapai = gpd.read_file(shapefile_path_Yavapai)\n",
    "\n",
    "print(sorted(gdf_Yavapai['PRECINCT']))\n",
    "plot_precincts(gdf_Yavapai, 'PRECINCT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yavapai_rows = final_df[final_df['county'].str.lower() == 'yavapai']\n",
    "yavapai_rows['precinct'] = yavapai_rows['precinct'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "print(sorted(yavapai_rows['precinct']))\n",
    "print(yavapai_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the 'precinct' values to keep only letters and convert to uppercase\n",
    "yavapai_rows['precinct'] = yavapai_rows['precinct'].str.replace(r'[^a-zA-Z\\s]', '', regex=True).str.upper().str.strip()\n",
    "gdf_Yavapai['PRECINCT'] = gdf_Yavapai['PRECINCT'].str.upper().str.strip()\n",
    "\n",
    "gdf_Yavapai_final = yavapai_rows.merge(gdf_Yavapai[['PRECINCT', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='PRECINCT', \n",
    "                                       how='right')\n",
    "\n",
    "# gdf_Yavapai_final = gdf_Yavapai_final.drop(columns=['PRECINCT'])\n",
    "\n",
    "gdf_Yavapai_final = gpd.GeoDataFrame(gdf_Yavapai_final, geometry='geometry')\n",
    "\n",
    "\n",
    "# Plot the precincts with labels\n",
    "plot_precincts(gdf_Yavapai_final, 'PRECINCT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yuma County\n",
    "Using 2020 boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuma_rows = final_df[final_df['county'].str.lower() == 'yuma']\n",
    "yuma_rows['precinct'] = yuma_rows['precinct'].str.split('.').str[0]\n",
    "yuma_rows.loc[yuma_rows['precinct'] == '25', 'precinct'] = '025'\n",
    "print(yuma_rows)\n",
    "print(len(yuma_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path_Yuma = './precinct_boundaries/yuma_2022/fwd2022voterprecinctmapasshapefiles/Voting_Pricincts_03_2023.shp'\n",
    "gdf_Yuma_2022 = gpd.read_file(shapefile_path_Yuma)\n",
    "\n",
    "print(gdf_Yuma_2022)\n",
    "plot_precincts(gdf_Yuma_2022, 'Precinct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking with voter file on L2, it is confirmed that districts 21 and 44 have no voters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_Yuma_final = yuma_rows.merge(gdf_Yuma_2022[['CODE', 'geometry']], \n",
    "                                       left_on='precinct', \n",
    "                                       right_on='CODE', \n",
    "                                       how='right')\n",
    "\n",
    "# Drop the duplicate UNIQUE_ID column from the merged DataFrame\n",
    "gdf_Yuma_final = gdf_Yuma_final.drop(columns=['CODE'])\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(gdf_Yuma_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine county level PBER into state level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# List of GeoDataFrames\n",
    "gdf_list = [\n",
    "    gdf_Apache_final, gdf_Cochise_final, gdf_Cococino_final, gdf_Gila_final,\n",
    "    gdf_Graham_final, gdf_Greenlee_final, gdf_Maricopa_final, gdf_La_Paz_final,\n",
    "    gdf_Mohave_final, gdf_Navajo_final, gdf_Pima_final, gdf_Pinal_final,\n",
    "    gdf_Santa_Cruz_final, gdf_Yavapai_final, gdf_Yuma_final\n",
    "]\n",
    "\n",
    "target_crs = \"EPSG:4326\"\n",
    "valid_gdfs = []\n",
    "\n",
    "for gdf in gdf_list:\n",
    "    if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "        try:\n",
    "            gdf = gpd.GeoDataFrame(gdf, geometry='geometry')\n",
    "        except KeyError:\n",
    "            print(f\"Missing geometry column in DataFrame: {gdf}\")\n",
    "            continue\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        gdf.set_crs(target_crs, inplace=True)\n",
    "\n",
    "    valid_gdfs.append(gdf.to_crs(target_crs))\n",
    "\n",
    "# Combine reprojected GeoDataFrames into a single GeoDataFrame\n",
    "combined_gdf = pd.concat(valid_gdfs, ignore_index=True)\n",
    "\n",
    "combined_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns\n",
    "def remove_duplicated_columns(gdf):\n",
    "    duplicates = gdf.columns[gdf.columns.duplicated()].unique()\n",
    "    gdf = gdf.loc[:, ~gdf.columns.duplicated()]\n",
    "    return gdf\n",
    "\n",
    "# Clean field names to ensure they are unique and valid for shapefiles\n",
    "def clean_field_names(gdf):\n",
    "    new_columns = {}\n",
    "    seen_columns = set()\n",
    "    for col in gdf.columns:\n",
    "        new_col = ''.join([c if ord(c) < 128 else '' for c in col])[:10]\n",
    "        # Ensure the column name is unique\n",
    "        if new_col in seen_columns:\n",
    "            counter = 1\n",
    "            new_col_temp = new_col\n",
    "            while new_col_temp in seen_columns:\n",
    "                new_col_temp = f\"{new_col[:8]}{counter}\"\n",
    "                counter += 1\n",
    "            new_col = new_col_temp\n",
    "        seen_columns.add(new_col)\n",
    "        new_columns[col] = new_col\n",
    "    gdf.rename(columns=new_columns, inplace=True)\n",
    "    return gdf\n",
    "\n",
    "# Remove duplicated columns\n",
    "combined_gdf = remove_duplicated_columns(combined_gdf)\n",
    "\n",
    "# Clean the field names in your GeoDataFrame\n",
    "cleaned_gdf = clean_field_names(combined_gdf)\n",
    "\n",
    "# Check for duplicated columns after cleaning\n",
    "if cleaned_gdf.columns.duplicated().any():\n",
    "    print(\"There are still duplicated column names after cleaning.\")\n",
    "else:\n",
    "    print(\"No duplicated columns after cleaning.\")\n",
    "\n",
    "# Save the GeoDataFrame to a shapefile\n",
    "cleaned_gdf.to_file(\"combined_gdf.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path_final = './combined_gdf.shp'\n",
    "shapefile_final = gpd.read_file(shapefile_path_final)\n",
    "\n",
    "ax = shapefile_final.plot()\n",
    "\n",
    "# Create a larger plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Plot the geometries\n",
    "shapefile_final.plot(ax=ax)\n",
    "\n",
    "# Add labels to each geometry\n",
    "for x, y, label in zip(shapefile_final.geometry.centroid.x, shapefile_final.geometry.centroid.y, shapefile_final['county']):\n",
    "    ax.text(x, y, label, fontsize=5, ha='center')  # Adjust font size as needed\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
